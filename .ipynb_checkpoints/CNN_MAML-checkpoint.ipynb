{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import numpy as np\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "data = np.load('data/RESISC45_images_96.npy')\n",
    "labels = np.load('data/RESISC45_classes.npy')\n",
    "\n",
    "test_size = 0.25\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data, labels, test_size = test_size, stratify = labels)\n",
    "\n",
    "\n",
    "np.save('data/RESISC45_images_train.npy', xtrain)\n",
    "np.save('data/RESISC45_labels_train.npy', ytrain)\n",
    "np.save('data/RESISC45_images_test.npy', xtest)\n",
    "np.save('data/RESISC45_labels_test.npy', ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load('data/RESISC45_images_train.npy')\n",
    "train_labels = np.load('data/RESISC45_labels_train.npy')\n",
    "classes = np.load('data/RESISC45_class_names.npy')\n",
    "\n",
    "print('Training data shape: ', train_data.shape)\n",
    "print('Testing data shape: ', train_labels.shape)\n",
    "print('Num Classes', classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = train_data.shape[2]# can use this to mofidy data size to fit this model (which only takes 256 images)\n",
    "\n",
    "bs = 32 # 64\n",
    "\n",
    "c_dim = classes.shape[0]\n",
    "\n",
    "print(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xval, ytrain, yval = train_test_split(train_data, train_labels, test_size = 0.25)\n",
    "\n",
    "xtrain = torch.tensor(xtrain).permute(0,3,1,2)\n",
    "print(torch.min(xtrain), torch.max(xtrain))\n",
    "\n",
    "trainset = []\n",
    "for i in range(xtrain.shape[0]):\n",
    "    trainset.append((xtrain[i], ytrain[i]))\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
    "                                          shuffle=False) #BUG: must keep shuffle false - or else it screws up labels, apparently\n",
    "\n",
    "## Validation Data\n",
    "valset = []\n",
    "xval = torch.tensor(xval).permute(0,3,1,2)\n",
    "\n",
    "print(torch.min(xval), torch.max(xval))\n",
    "for i in range(xval.shape[0]):\n",
    "    valset.append((xval[i], yval[i]))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=64, drop_last = True,\n",
    "                                          shuffle=False) #BUG: must keep shuffle false - or else it screws up labels, apparently\n",
    "\n",
    "test_data = np.load('data/RESISC45_images_test.npy')\n",
    "test_labels = np.load('data/RESISC45_labels_test.npy')\n",
    "\n",
    "test_data = torch.tensor(test_data)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "## Testing Data\n",
    "testset = []\n",
    "print(test_data.shape)\n",
    "xtest = torch.tensor(test_data).permute(0,3,1,2)\n",
    "print(xtest.shape)\n",
    "\n",
    "print(torch.min(test_data), torch.max(test_data))\n",
    "for i in range(test_data.shape[0]):\n",
    "    testset.append((test_data[i], test_labels[i]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, drop_last = True,\n",
    "                                          shuffle=False) #BUG: must keep shuffle false - or else it screws up labels, apparently\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of a batch of images:')\n",
    "print(next(iter(train_loader))[0].shape)\n",
    "print('Shape of a batch of labels:')\n",
    "print(next(iter(train_loader))[1].shape)\n",
    "\n",
    "first_samp = next(iter(train_loader))[0][0] #get first sample in first batch\n",
    "print(torch.min(first_samp), torch.max(first_samp)) #images naturally (0,255)\n",
    "plt.imshow(first_samp.permute(1,2,0)/255) #show it\n",
    "\n",
    "name = next(iter(train_loader))[1][0].data.item()\n",
    "print(name)\n",
    "print(classes[name]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Pred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Conv_Pred, self).__init__()\n",
    "        ## Encoding: Unconditional samples\n",
    "        self.conv1 = nn.Conv2d(3, 32, 4, 2, 1) # Input: (bs, 3, img_size, img_size)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1, bias = False)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, 2, 1, bias = False)\n",
    "        self.conv3_bn = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, 2, 1, bias = False)\n",
    "        self.conv4_bn = nn.BatchNorm2d(256)\n",
    "        self.conv5 = nn.Conv2d(256, 512, 4, 2, 1, bias = False)\n",
    "        self.conv5_bn = nn.BatchNorm2d(512)\n",
    "        self.conv6 = nn.Conv2d(512, 1024, 4, 2, 1, bias = False)\n",
    "        self.conv6_bn = nn.BatchNorm2d(1024)\n",
    "        \n",
    "        #self.conv7 = nn.Conv2d(2048, z_dim, 4, 2, 0) # Output: (bs, c_dim, 1, 1)\n",
    "        self.fce = nn.Linear(1024, 45)\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode data x to 2 spaces: condition space and variance-space\n",
    "        x = F.relu(self.conv1(x), 0.2)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
    "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
    "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
    "        x = F.relu(self.conv6_bn(self.conv6(x)))\n",
    "\n",
    "        z = nn.Softmax(dim=1)(self.fce(x.squeeze()))\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "def normal_init(m):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "        #m.bias.data.zero_()\n",
    "\n",
    "def one_hot_embedding(labels):\n",
    "    labels = torch.nn.functional.one_hot(torch.tensor(labels).to(torch.int64), num_classes = c_dim)\n",
    "    return torch.squeeze(labels)\n",
    "\n",
    "def top_k_acc(inp, targ, k):\n",
    "    #print(inp.shape)\n",
    "    tops = torch.topk(inp, k=k, dim = 1)\n",
    "    \n",
    "    i = 0\n",
    "    corrects = 0\n",
    "    for row in tops:\n",
    "        for element in row:\n",
    "            if element == targ[i]:\n",
    "                corrects += 1\n",
    "            \n",
    "        i+=1\n",
    "        \n",
    "    return corrects / inp.shape[0]\n",
    "\n",
    "def accuracy_topk(output, target, topk=(3,)):\n",
    "    #https://forums.fast.ai/t/return-top-k-accuracy/27658\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = Conv_Pred()\n",
    "CNN.weight_init()\n",
    "CNN.to(device)\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "mse_loss = nn.MSELoss()\n",
    "mae_loss = nn.L1Loss()\n",
    "learning_rate = 0.0006\n",
    "\n",
    "CNN_optimizer = optim.Adam(CNN.parameters(),\n",
    "                         lr = learning_rate)\n",
    "                         #betas = (beta_1, beta_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CNN_loss_tracker = []\n",
    "val_accs, val_topks = [], []\n",
    "n_epochs = 25 # 25+ needed. just keep raising this number...\n",
    "print_stride = n_epochs // 5\n",
    "\n",
    "import augmentation_functions\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    CNN_losses = []\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        task_list = [random.randrange(0,128) for _ in range(3)] # sample 3 tasks\n",
    "        print(task_list)\n",
    "        y = one_hot_embedding(y.to(device)).float()\n",
    "        \n",
    "        X = 2.0*(X/255 - 0.5)\n",
    "        mini_batch = X.size()[0]\n",
    "        X = X.to(device).float()\n",
    "\n",
    "        for task in task_list:\n",
    "            task_batch = augmentation_functions.applyAugs(X, task)\n",
    "            # plt.imshow(taskbatch) to debug and make sure scott adam works\n",
    "            #task_batch = task_batch.to(device).float()\n",
    "            \n",
    "            \n",
    "\n",
    "            ## CNN Training\n",
    "            for param in CNN.parameters():\n",
    "                param.grad = None\n",
    "                \n",
    "            #print(task_batch.shape)\n",
    "\n",
    "            yhat = CNN(task_batch)\n",
    "            #print(yhat.shape, y.shape)\n",
    "            pred_loss = bce_loss(yhat, y)\n",
    "            \n",
    "            grads=torch.autograd.grad(inner_loss, model.meta_params(), create_graph=True)\n",
    "            params = OrderedDict()\n",
    "            for (name,param), grad in zip(model.meta_named_pars(), grads()):\n",
    "                params[name] = params- step_size * grad\n",
    "            ## we need from this x,y in task batch we need to partition this into a inner loop and outer loop\n",
    "            # sets of x and y. They must have gone through the same augmentations to correspond to the same task\n",
    "            ## we wish to compute the outer loss function for this task using the params\n",
    "            # from this task, the model has not actuelly been updated yet,\n",
    "            # all paramaters are updated after we call backwards() on the outer loss\n",
    "            outer_logit = model(outer_task_batch_x, params=params)\n",
    "            outer_loss += F.cross_entropy(outer_logit,outer_task_batch_y) ## add this loss for these params to the outer loop. Note\n",
    "            # no update the the model has been done so each task has the same initialization\n",
    "            ## note using torch here allows us do do compute gradient once or for loop for 1 loop for 1 gradient update for each task\n",
    "            ## torch meta has the ability to do a for loop for n loops where we take the gradient n times\n",
    "\n",
    "            #model_loss = pred_loss\n",
    "\n",
    "        outer_loss.backward()\n",
    "        CNN_optimizer.step()\n",
    "\n",
    "        CNN_losses.append(pred_loss.data.item())\n",
    "        \n",
    "    \n",
    "    if epoch % print_stride == 0:\n",
    "        print('Epoch {} - loss_CNN: {:.3f}'.format((epoch), torch.mean(torch.FloatTensor(CNN_losses))))\n",
    "\n",
    "        CNN_loss_tracker.append(torch.mean(torch.FloatTensor(CNN_losses)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            accs, actk = [], []\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(device).float(), y.to(device).float()\n",
    "                #print(x, y)\n",
    "                yhat = CNN(x)\n",
    "                \n",
    "                yhat_max = torch.max(yhat, dim = 1)[1]\n",
    "                #print(yhat.shape)\n",
    "                \n",
    "                correct = torch.sum(yhat_max == y)\n",
    "                size = x.shape[0]\n",
    "                \n",
    "                acc_topk = accuracy_topk(yhat, y)\n",
    "                #print(acc_topk)\n",
    "                actk.append(acc_topk.data.item())\n",
    "                \n",
    "                accs.append(100*(correct/size).data.item())\n",
    "            print()\n",
    "            print('Validation Accuracy: ', torch.mean(torch.FloatTensor(accs)).data.item())\n",
    "            print('Validation Top3 Accuracy: ', torch.mean(torch.FloatTensor(actk)).data.item())\n",
    "            print()\n",
    "            \n",
    "        val_accs.append(torch.mean(torch.FloatTensor(accs)))\n",
    "        val_topks.append(torch.mean(torch.FloatTensor(actk)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(CNN_loss_tracker, label = 'train loss')\n",
    "plt.plot(val_accs, label = 'val acc')\n",
    "plt.plot(val_topks, label = 'val top3')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    accs, topks = [], []\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.permute(0,3,1,2).to(device).float(), y.to(device).float()\n",
    "        #print(x, y)\n",
    "        yhat = CNN(x)\n",
    "\n",
    "        yhat_max = torch.max(yhat, dim = 1)[1]\n",
    "        #print(yhat.shape)\n",
    "\n",
    "        correct = torch.sum(yhat_max == y)\n",
    "        size = x.shape[0]\n",
    "        acc_topk = accuracy_topk(yhat, y)\n",
    "        actk.append(acc_topk.data.item())\n",
    "                \n",
    "        accs.append(100*(correct/size).data.item())\n",
    "        topks.append(actk)\n",
    "\n",
    "    print('Test Accuracy: ', torch.mean(torch.FloatTensor(accs)).data.item())\n",
    "    print('Test Top3 Accuracy: ', torch.mean(torch.FloatTensor(topks)).data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
