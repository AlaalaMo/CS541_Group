# -*- coding: utf-8 -*-
"""CS541_Project_MAML_and_CNN_no_filter_second_order.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GWcjKDYMbwrdzDXp0bTMw94MhYaBFZIM
"""

!pip install learn2learn
!pip install torchmeta
import torch
import matplotlib.pyplot as plt
from google.colab import drive
import time
import random
import torch
import numpy as np
import learn2learn as l2l
import math
import torch.utils
from torch.utils.data import Dataset,DataLoader
import random
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
drive.mount('/content/drive')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

Params = {'nways': 32, 'kshots': 1, 'in_channels': 3, 'hidden_size': 2, 'innerStep': 0.00005**5/16, 'MetaLR': 0.006*5/64, 'number_of_tasks': 32,
                  'Order': False, 'outerVSinner': 1, 'epoch': 150, 'aug': True, 'Img_size': 96, 
                   'num_classes': 45,
}

class CustomTensorDataset(Dataset):

    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform=transform

    def __getitem__(self, index):
        X = self.images[index]
        y = self.labels[index]
        return X, y

    def __len__(self):
        return len(self.images)

def make_dataset(x, y):
    x = torch.tensor(x).permute(0,3,1,2)
    y = torch.tensor(y)
    dset = CustomTensorDataset(x, y)
    return dset
def linkDataset(x,y):
    set = []
    for i in range(len(y)):
        set.append((x[i], y[i]))
    return set
def get_data(dir, mtype,Params,nwaykshot=False):
    if True: batch_n =32*2
    data = np.load(dir+'RESISC45_images_96.npy')
    labels = np.load(dir+'RESISC45_classes.npy')
    classes = np.load(dir+'RESISC45_class_names.npy')
    c_dim = classes.shape[0]

    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = .8, stratify = labels)

    xtrain, xval, ytrain, yval = train_test_split(train_data, train_labels, test_size = .2)
    trainset = make_dataset(xtrain,ytrain)
    if nwaykshot == True:
        train_loader = taskStructure(trainset,Params,ytrain)
    else:
        train_loader = DataLoader(trainset, batch_size=int(batch_n), shuffle=True)


    valset = make_dataset(xval, yval)
    val_loader = torch.utils.data.DataLoader(valset, batch_n, drop_last = True, shuffle=True)

    testset = make_dataset(test_data, test_labels)
    test_loader = torch.utils.data.DataLoader(testset, batch_n, drop_last = True, shuffle=True)
    return train_loader, val_loader, test_loader, c_dim, classes, Params,trainset,ytrain



def taskStructure(dataset,Params,train_labels): ## pass train into here
    labels= {}
    for i in np.unique(ytrain):
        labels[i] = [j for j, x in enumerate(train_labels) if x == i]
    metaDataset = l2l.data.MetaDataset(dataset,labels_to_indices=labels)
    
    transforms = [
    ## need kshots*2 for inner and outer loop split accross each task
        l2l.data.transforms.FusedNWaysKShots(metaDataset,n=Params['nways'],k=Params['kshots']*2,replacement=True,filter_labels= np.unique(ytrain).tolist()),
        l2l.data.transforms.LoadData(metaDataset),
        l2l.data.transforms.ConsecutiveLabels(metaDataset)
    ]
    taskset = l2l.data.TaskDataset(metaDataset,
                                   transforms,
                                   num_tasks=Params['lenx']/(Params['nways']*Params['kshots']*2))
                                    ## generate max number of tasks
        
    return taskset
def tasksplit_uneven(x,y,Params):
    ## split inner and outer loop by incidies
    skip = 2
    idx = idx = torch.arange(start =1, end=x.shape[0]+1).bool()
    idx[::skip] = False
    x_inner = x[idx]
    y_inner = y[idx]
    x_outer = x[~idx]
    y_outer = y[~idx]
    return x_inner, x_outer, y_inner,y_outer

train,val,test, cdim, classes,Params,trainset,ytrain = get_data('/content/drive/MyDrive/Project/data/',mtype='MAMLnk',Params=Params,nwaykshot=False)

# -*- coding: utf-8 -*-
"""augmentation_functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SfjUcITKHjZqWWSwe-yNoI0aD1JIggj2
"""

import numpy as np
import itertools
import torch
import torchvision
from torchvision import transforms
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class addGaussianNoise(object):
    def __init__(self, mean=0.0, std=1.0, p=0.5):
        self.mean = torch.tensor(mean).to(device)
        self.std = torch.tensor(std).to(device)
        self.p = p
      
    def __call__(self, img):
        if torch.rand(1).item() < self.p:
            return img + torch.randn(img.size(), device = device) * self.std + self.mean
        return img
        
    def __repr__(self):
        return self.__class__.__name__ + '(mean={0}, std={1}, p={2})'.format(self.mean, self.std, self.p)

def applyAugs(img_batch, task_idx, num_augs=10):
    # returns augmented batch of images based on task index (0:128)
    # currently based on exactly 7 transforms 

    transform_list = [transforms.RandomHorizontalFlip(p=0.99),
                      transforms.RandomVerticalFlip(p=0.99),
                      transforms.RandomRotation((120,120), fill=0.5),
                      transforms.RandomRotation((240,240), fill=0.5),
                      transforms.RandomRotation((75,75), fill=0.5),
                      transforms.RandomRotation((300,300), fill=0.5),
                      transforms.RandomPerspective(distortion_scale=0.1, p=0.99, fill=0.5),
                      transforms.RandomResizedCrop(96,
                                                   scale=(0.5, 1.0),
                                                   ratio=(0.8, 1.0)),
                                                   #interpolation=transforms.InterpolationMode.BILINEAR),
                      addGaussianNoise(std=0.1, p=0.99),
                      # transforms.ColorJitter(saturation=4.0, hue=0.01),
                      transforms.ColorJitter(brightness=0.5, contrast=0.9)
                      # ,transforms.GaussianBlur(9, sigma=(0.01, 2.0))
                      ]
       
    tasklist = list(itertools.product([0, 1], repeat=num_augs))
    current_augs = tasklist[task_idx]

    task_transforms = [transform_list[i] for i,x in enumerate(current_augs) if x==1]
    transform = torchvision.transforms.Compose(task_transforms)
    img_batch = transform(img_batch)
    return img_batch

def getAugmentationTransforms(task_idx, num_augs=10):
    # returns transforms.Compose function of transforms based on task index (0:128)
    # currently based on exactly 7 transforms 

    transform_list = [transforms.RandomHorizontalFlip(p=0.99),
                      transforms.RandomVerticalFlip(p=0.99),
                      transforms.RandomRotation(359.0, fill=0.5),
                      transforms.RandomPerspective(distortion_scale=0.1, p=0.99, fill=0.5),
                      transforms.RandomResizedCrop(256,
                                                   scale=(0.5, 1.0),
                                                   ratio=(1.0, 1.0),
                                                   interpolation=transforms.InterpolationMode.BILINEAR),
                      addGaussianNoise(std=0.1, p=0.99),
                      # transforms.ColorJitter(saturation=4.0, hue=0.01),
                      transforms.ColorJitter(brightness=0.5, contrast=0.9)
                      # ,transforms.GaussianBlur(9, sigma=(0.01, 2.0))
                      ]
       
    tasklist = list(itertools.product([0, 1], repeat=num_augs))
    current_augs = tasklist[task_idx]

    task_transforms = [transform_list[i] for i,x in enumerate(current_augs) if x==1]
    transform = torchvision.transforms.Compose(task_transforms)

    return transform


def normalizeImages(x):
  x = x/255.
  return x

def convertToTorch(x):
  x = np.moveaxis(x, 3, 1)
  x = torch.as_tensor(x)
  return x

def convertToNumpy(x):
  # convert back to format for displaying
  x = x.numpy()
  x = np.moveaxis(x, 1, 3)  
  return x

import torch.nn as nn
from torchmeta.modules import (MetaModule, MetaSequential, MetaConv2d,
                               MetaBatchNorm2d, MetaLinear)



class ConvolutionalNeuralNetwork(MetaModule):
    def __init__(self,Params,in_channels=3,out_features=45,hidden_size=2):
        super(ConvolutionalNeuralNetwork, self).__init__()
        self.in_channels = Params['in_channels']
        self.out_features = Params['num_classes']
        self.hidden_size = Params['hidden_size']

        self.features  = MetaSequential(
            MetaConv2d(in_channels,hidden_size**5,kernel_size=4,stride=2,padding=1),
            nn.ReLU(),
            MetaConv2d(hidden_size**5,hidden_size**6,kernel_size=4,stride=2,padding=1,bias=False),
            MetaBatchNorm2d(hidden_size**6),
            nn.ReLU(),
            MetaConv2d(hidden_size ** 6, hidden_size ** 7, kernel_size=4, stride=2, padding=1, bias=False),
            MetaBatchNorm2d(hidden_size**7),
            nn.ReLU(),
            MetaConv2d(hidden_size ** 7, hidden_size ** 8, kernel_size=4, stride=2, padding=1, bias=False),
            MetaBatchNorm2d(hidden_size ** 8),
            nn.ReLU(),
            MetaConv2d(hidden_size ** 8, hidden_size ** 9, kernel_size=4, stride=2, padding=1, bias=False),
            MetaBatchNorm2d(hidden_size ** 9),
            nn.ReLU(),
            MetaConv2d(hidden_size ** 9, hidden_size ** 10, kernel_size=4, stride=2, padding=1, bias=False),
            MetaBatchNorm2d(hidden_size ** 10),
            nn.ReLU(),
        )
        self.classifier = MetaLinear(self.hidden_size**10, out_features)



    def weight_init(self):
        for m in self._modules:
            normal_init(self._modules[m])
    def forward(self, inputs, params=None):
        features = self.features(inputs, params=self.get_subdict(params, 'features'))
        features = features.view((features.size(0), -1))
        logits = self.classifier(features, params=self.get_subdict(params, 'classifier'))
        return logits
        
    def save(self, name):
        print('saving...')
        torch.save({
            'model_state_dict': self.state_dict() #,
            #'optimizer_state_dict': self.optimizer.state_dict(),
        }, dir+name+'.pt')

    def load(self, name):
        print('loading...')
        checkpoint = torch.load(dir+name+'.pt')
        self.load_state_dict(checkpoint['model_state_dict'])
def accuracy_topk(output, target, topk=(3,)):
    # https://forums.fast.ai/t/return-top-k-accuracy/27658
    """Computes the precision@k for the specified values of k"""
    maxk = max(topk)
    batch_size = target.size(0)
    _, pred = output.topk(maxk, 1, True, True)
    pred = pred.t()
    correct = pred.eq(target.reshape(1, -1).expand_as(pred))
    res = []
    for k in topk:
        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
        res.append(correct_k.mul_(1 / batch_size))
    return res[0]

from torchmeta.utils.gradient_based import gradient_update_parameters
import torch.nn.functional as F


def trainMAML(model, train_tasks, Params, val_loader,nwaykshot=False, trainset=None,ytrain=None):  ## go through data set
    epoch_tracker, loss_tracker, val_accs, val_topks,trainacc = [], [], [], [],[]
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.train()
    model.to(device)
      # keeping track of tasks seen make sure to see entrie dataset
    for epoch in range(1, Params['epoch']+1):
        tracc, trlosses= looptrain(Params,model,train,nwaykshot,trainset=trainset)
        trainacc.append(tracc)
        loss_tracker.append(trlosses)
        with torch.no_grad():
            accs, actk = [], []
            for x, y in val_loader:
                x, y = x.to(device).float(), y.to(device).float()
                yhat = model(x)
                
                yhat_max = torch.max(yhat, dim = 1)[1]
                
                correct = torch.sum(yhat_max == y)
                size = x.shape[0]
                
                acc_topk = accuracy_topk(yhat, y)
                actk.append(acc_topk.data.item())
                
                accs.append(100*(correct/size).data.item())
                ac = torch.mean(torch.FloatTensor(accs)).data.item()
                top = torch.mean(torch.FloatTensor(actk)).data.item()
            print('On Epoch ',epoch)
            print('Train Loss', loss_tracker[-1])
            print('Validation Accuracy: ', ac)
            print('Validation Top3 Accuracy: ', top)
            val_accs.append(ac)
            val_topks.append(top*100)
            epoch_tracker.append(epoch)
        ac = torch.mean(torch.FloatTensor(accs)).data.item()
        top = torch.mean(torch.FloatTensor(actk)).data.item()
        if epoch%20==0:
            Params['MetaLR']*=.5
            Params['innerStep']*=.5
    print_stride1=1

    return epoch_tracker[::print_stride1], loss_tracker[::print_stride1], val_accs[::print_stride1], val_topks[::print_stride1], trainacc[::print_stride1], model


def looptrain(Params,model,train_loader,nwaykshot=False,trainset=None):
    trainacc,trainloss = [], []
    if nwaykshot==True:
        train_loader = taskStructure(trainset,Params,ytrain)
        print(nwaykshot)
    shape=0
    j = 0
    meta_optimizer = torch.optim.Adam(model.parameters(), lr=Params['MetaLR'])
    outer_loss = torch.tensor(0., device=device)
    for task in train_loader:
        i = torch.randint(high=(2^10-1), size=(1, 1)).item()
        model.zero_grad()
        loss_amount = 0 
        ## call augments on the fly
        x, y = task
        shape+= y.shape[0]
        x = x.float().to(device)
        y = y.long().to(device)
        x_inner, x_outer, y_inner, y_outer = tasksplit_uneven(applyAugs(x, i), y,
                                                        Params)  ## split task for inner/ ouyter with same augs
        train_logit = model(x_inner)
        inner_loss = F.cross_entropy(train_logit, y_inner)
        loss_amount += inner_loss.item()
        model.zero_grad()
        params = gradient_update_parameters(model,
                                            inner_loss,
                                            params=None,
                                            step_size=Params["innerStep"],
                                            first_order=Params['Order'])
        test_logit = model(x_outer,
                        params=params)  ## take the loss fucntions using the params of this task specific inner loop
        current_outer_loss = F.cross_entropy(test_logit, y_outer)
        outer_loss += current_outer_loss ## sum this inot the outer loop
        current_outer_loss.div_(len(x_outer))
        
        yhat_max = torch.max(test_logit, dim = 1)[1]
        correct = torch.sum(yhat_max == y_outer)
        size = x.shape[0]
        acc = 100*(correct/size).data.item()
        trainacc.append(acc)
        if j % Params['number_of_tasks'] == 0:  ## we hit number of tasks if this =0 then we do the outer loop with the update as a sum 
            outer_loss.backward()
            loss_amount += outer_loss.item()
            meta_optimizer.step()
            outer_loss = torch.tensor(0., device=device)
            for param in model.parameters(): param.grad = None
        elif outer_loss.item()!= 0 and 5040 -shape< 2*Params['nways']*Params['kshots']+33: ## do not waste those computations!
            outer_loss.backward()
            loss_amount += outer_loss.item()
            meta_optimizer.step()
            outer_loss = torch.tensor(0., device=device)
            for param in model.parameters(): param.grad = None
        j += 1
    meta_optimizer.zero_grad()
    model.zero_grad()
    train_accuracy = torch.mean(torch.FloatTensor(trainacc)).data.item()
    train_losses = loss_amount/shape
    return train_accuracy, train_losses

model = ConvolutionalNeuralNetwork(Params)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
epoch_tracker, loss_tracker, val_accs, val_topks, trainacc,modelnk = trainMAML(model, train, Params, val,trainset,ytrain,False)

dir = '/content/drive/MyDrive/Project/data/'
class CustomTensorDataset(Dataset):

    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform=transform

    def __getitem__(self, index):
        X = self.images[index]
        y = self.labels[index]
        return X, y

    def __len__(self):
        return len(self.images)

def make_dataset(x, y):
    x = torch.tensor(x).permute(0,3,1,2)
    y = torch.tensor(y)
    dset = CustomTensorDataset(x, y)
    return dset
def UC_get_data():
    UC_MAML_learning_rate = 0.0006
    UC_CNN_learning_rate = 0.0006
    UC_epochs = 3
    UC_batch_size = 64
    UC_data = np.load(dir+'UCMerced_images96.npy')
    UC_labels = np.load(dir+'UCMerced_classes.npy')
    UC_classes = np.load(dir+'UCMerced_class_names.npy')

    UC_train_data, UC_test_data, UC_train_labels, UC_test_labels = train_test_split(UC_data, UC_labels, test_size = 0.8, stratify = UC_labels)

    UCtrainset = make_dataset(UC_train_data, UC_train_labels)
    UCtrain_loader = DataLoader(UCtrainset, batch_size=int(UC_batch_size), shuffle=True)

    UCtestset = make_dataset(UC_test_data, UC_test_labels)
    UCtest_loader = torch.utils.data.DataLoader(UCtestset, UC_batch_size, drop_last = True, shuffle=True)
    return UCtrain_loader, UCtest_loader
UC_train_loader , UC_test_loader = UC_get_data()

import copy
def MAMLUC(model1,train_loader,test_loader):
    epoch_tracker, loss_tracker, val_accs, val_topks,trainacc = [], [], [], [],[]
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = copy.deepcopy(model1)
    model.train()
    model.to(device)
    for epoch in range(1,4):

        trainacc,trainloss = [], []
        shape=0
        j = 0
        meta_optimizer = torch.optim.Adam(model.parameters(), lr=.0006)
        outer_loss = torch.tensor(0., device=device)
        for task in train_loader:
            model.zero_grad()
            ## call augments on the fly
            x, y = task
            shape+= y.shape[0]
            x = x.float().to(device)
            y = y.long().to(device)
            x_inner, x_outer, y_inner, y_outer = tasksplit_uneven(x, y,
                                                            Params)  ## split task for inner/ ouyter with same augs
            train_logit = model(x_inner)
            inner_loss = F.cross_entropy(train_logit, y_inner)
            model.zero_grad()
            params = gradient_update_parameters(model,
                                                inner_loss,
                                                params=None,
                                                step_size=.0006,
                                                first_order=True)
            test_logit = model(x_outer,
                            params=params)  ## take the loss fucntions using the params of this task specific inner loop
            current_outer_loss = F.cross_entropy(test_logit, y_outer)
            outer_loss += current_outer_loss ## sum this inot the outer loop
            current_outer_loss.div_(len(x_outer))
            
            yhat_max = torch.max(test_logit, dim = 1)[1]
            correct = torch.sum(yhat_max == y_outer)
            size = x.shape[0]
            acc = 100*(correct/size).data.item()
            trainacc.append(acc)
            outer_loss.backward()
            meta_optimizer.step()
            outer_loss = torch.tensor(0., device=device)
            for param in model.parameters(): param.grad = None
            del params
            meta_optimizer.zero_grad()
        
        train_accuracy = torch.mean(torch.FloatTensor(trainacc)).data.item()
        trainacc.append(train_accuracy)
        ## print out testing data
        with torch.no_grad():
            testaccs, testactk = [], []
            for x, y in test_loader:
                x, y = x.to(device).float(), y.to(device).float()
                yhat = model(x)
                
                yhat_max = torch.max(yhat, dim = 1)[1]
                
                correct = torch.sum(yhat_max == y)
                size = x.shape[0]
                
                acc_topk = accuracy_topk(yhat, y)
                testactk.append(acc_topk.data.item())
                
                testaccs.append(100*(correct/size).data.item())
                ac = torch.mean(torch.FloatTensor(testaccs)).data.item()
                top = torch.mean(torch.FloatTensor(testactk)).data.item()
            print('On Epoch ',epoch)
            print('UC Test Accuracy: ', ac)
            print('UC Test Top3 Accuracy: ', top)
            val_accs.append(ac)
            val_topks.append(top*100)
            epoch_tracker.append(epoch)

    return epoch_tracker, loss_tracker, val_accs, val_topks,trainacc

def UC_test_runs(model1,train_loader,test_loader,runs):
    acc,acct = 0,0
    for _ in range(runs):
         epoch_tracker, loss_tracker, val_accs, val_topks,trainacc = MAMLUC(model1,train_loader,test_loader)
         acc += val_accs[-1]
         acct+=  val_topks[-1]
    print('Avg of 20 runs on UC test Acc and topk: ',acc/runs, acct/runs)
UC_test_runs(model,UC_train_loader , UC_test_loader,20)



