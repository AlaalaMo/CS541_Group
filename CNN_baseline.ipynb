{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "CNN_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RQuS27hBv1h"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import time\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXw4UdisBxts",
        "outputId": "2844cbcb-c91b-4964-9f17-b0f7f3148077"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "%cd '/gdrive/MyDrive/Project'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/.shortcut-targets-by-id/1ONXXjIHVMg4MHpAdUIdbB4Fr9W33JPYU/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2NANw0i8ndY"
      },
      "source": [
        "import augmentation_functions\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IYtFydSBv1o"
      },
      "source": [
        "\n",
        "class Conv_Pred(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_Pred, self).__init__()\n",
        "        ## Encoding: Unconditional samples\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1) \n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias = False)\n",
        "        self.conv2_bn = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias = False)\n",
        "        self.conv3_bn = nn.BatchNorm2d(32)\n",
        "        self.conv4 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias = False)\n",
        "        self.conv4_bn = nn.BatchNorm2d(32)\n",
        "        self.conv5 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias = False)\n",
        "        self.conv5_bn = nn.BatchNorm2d(32)\n",
        "        self.conv6 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias = False)\n",
        "        self.conv6_bn = nn.BatchNorm2d(32)\n",
        "        self.conv7 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1, bias = False)\n",
        "        self.conv7_bn = nn.BatchNorm2d(32)\n",
        "        \n",
        "        self.fce = nn.Linear(32*4, 45)\n",
        "\n",
        "    def weight_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode data x to 2 spaces: condition space and variance-space\n",
        "        # x = x/127.5 - 0.5\n",
        "        # x = x/255\n",
        "        x = F.relu(self.conv1(x), 0.2)\n",
        "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
        "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
        "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
        "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
        "        x = F.relu(self.conv6_bn(self.conv6(x)))\n",
        "        x = F.relu(self.conv7_bn(self.conv7(x)))\n",
        "        \n",
        "        #print(x.shape, self.fce.weight.shape)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        z = nn.Softmax(dim=1)(self.fce(x.squeeze()))\n",
        "        return z\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJ1ZNz-TjBqo"
      },
      "source": [
        "\n",
        "def normal_init(m):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        # m.bias.data.zero_()\n",
        "\n",
        "def one_hot_embedding(labels, c_dim):\n",
        "    labels = torch.nn.functional.one_hot(torch.tensor(labels).to(torch.int64), num_classes=c_dim)\n",
        "    return torch.squeeze(labels)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSkNfykQWi_c"
      },
      "source": [
        "\n",
        "def convert_images_numpy(images):\n",
        "    return images.permute(0,2,3,1).numpy()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jcNqnBOnimi"
      },
      "source": [
        "\n",
        "def convert_images_torch(images):\n",
        "    return torch.as_tensor(images).permute(0,3,1,2)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKwcwg44Ag__"
      },
      "source": [
        "\n",
        "def resize_images(images, resolution=256): \n",
        "    if type(images) == np.ndarray:\n",
        "      images = convert_images_torch(images)\n",
        "    \n",
        "    resized_images = torch.zeros([len(images), 3, resolution, resolution], dtype=torch.float32)\n",
        "    img_transforms = transforms.Compose([transforms.ToPILImage(),\n",
        "                                        transforms.Resize(size=resolution),\n",
        "                                        transforms.ToTensor()])\n",
        "    for i in range(images.shape[0]):\n",
        "      resized_images[i] = img_transforms(images[i])\n",
        "    return(resized_images)\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXczxDIlPcoK"
      },
      "source": [
        "\n",
        "def augment_dataset(dataset, labels, aug_size_pct, aug_list=[0,0,0,0,0,0,0], resolution=96): \n",
        "    # increase training data with augmented samples by 'aug_size_pct' of existing data size\n",
        "    # aug_list = binary toggles for applying any of 7 defined transforms\n",
        "\n",
        "    # select sample of data to augment\n",
        "    selected_data, skip_data, selected_labels, skip_labels = train_test_split(dataset, labels, \n",
        "                                                                              test_size=(1-aug_size_pct), \n",
        "                                                                              stratify=labels)\n",
        "    \n",
        "    # selected_data = convert_images_torch(selected_data)         \n",
        "    selected_data = resize_images(selected_data, 256).to(device)   # transforms want 256x256 resolution\n",
        "    augmented_data = torch.zeros(selected_data.shape).to(device)\n",
        "    \n",
        "    step_size = 32  # transform in batches to avoid memory issues\n",
        "    for i in range(0, selected_data.shape[0], step_size):\n",
        "      augmented_data[i:i+step_size] = augmentation_functions.applyAugsManual(selected_data[i:i+step_size], aug_list) \n",
        "    \n",
        "    augmented_data = resize_images(augmented_data, resolution) # downsample to original resolution\n",
        "    augmented_data = convert_images_numpy(augmented_data.to('cpu'))\n",
        "\n",
        "    new_dataset = np.concatenate((dataset, augmented_data))\n",
        "    new_labels = np.concatenate((labels, selected_labels))\n",
        "\n",
        "    return new_dataset, new_labels\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6I09AvfPGmE"
      },
      "source": [
        "\n",
        "def load_data_files(test_pct):\n",
        "    file = 'data/RESISC45_images_96.npy'\n",
        "    data = np.load(file)\n",
        "    print(\"image data:\", data.shape)\n",
        "    labels = np.load('data/RESISC45_classes.npy')\n",
        "    xtrain, xtest, ytrain, ytest = train_test_split(data, labels, test_size=test_pct, stratify=labels)\n",
        "\n",
        "    np.save('data/RESISC45_images_train.npy', xtrain)\n",
        "    np.save('data/RESISC45_labels_train.npy', ytrain)\n",
        "    np.save('data/RESISC45_images_test.npy', xtest)\n",
        "    np.save('data/RESISC45_labels_test.npy', ytest)\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7BShknuiww1"
      },
      "source": [
        "  \n",
        "def linkDataset(x,y):\n",
        "    set = []\n",
        "    for i in range(len(y)):\n",
        "        set.append((x[i], y[i]))\n",
        "    return set\n",
        "    \n",
        "def get_data(Params, load=False):\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "\n",
        "    train_data = np.load('data/RESISC45_images_train.npy')\n",
        "    train_labels = np.load('data/RESISC45_labels_train.npy')\n",
        "    classes = np.load('data/RESISC45_class_names.npy')\n",
        "\n",
        "    img_size = train_data.shape[2]  \n",
        "    print(\"img_size:\", img_size)\n",
        "    c_dim = classes.shape[0]\n",
        "    bs = Params['batch_size']\n",
        "\n",
        "    if (Params['dataset_pct'] < 1): # to use only subset of data for faster pre-testing\n",
        "      train_data, untrain_data, train_labels, untrain_labels = train_test_split(train_data, train_labels, \n",
        "                                                                                test_size=(1-Params['dataset_pct']), \n",
        "                                                                                stratify=train_labels)\n",
        "\n",
        "    xtrain, xval, ytrain, yval = train_test_split(train_data, train_labels, test_size=Params['test_pct'])\n",
        "\n",
        "    # remove to conserve memory\n",
        "    del train_data\n",
        "\n",
        "    ## Training Data\n",
        "    if (Params['aug_pct'] > 0):  # augment training examples\n",
        "      xtrain, ytrain = augment_dataset(xtrain, ytrain, aug_size_pct=Params['aug_pct'], aug_list=Params['aug_list'], resolution=img_size)\n",
        "\n",
        "    xtrain = torch.tensor(xtrain).permute(0, 3, 1, 2)\n",
        "    trainset = linkDataset(xtrain, ytrain)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=bs, drop_last=True, shuffle=True) \n",
        "    print('train data:', xtrain.shape)\n",
        "\n",
        "    ## Validation Data\n",
        "    valset = []\n",
        "    xval = torch.tensor(xval).permute(0, 3, 1, 2)\n",
        "    valset = linkDataset(xval, yval)\n",
        "    val_loader = torch.utils.data.DataLoader(valset, batch_size=bs, drop_last=True, shuffle=True) \n",
        "    print('val data:', xval.shape)\n",
        "\n",
        "    ## Testing Data\n",
        "    test_data = np.load('data/RESISC45_images_test.npy')\n",
        "    test_labels = np.load('data/RESISC45_labels_test.npy')\n",
        "\n",
        "    test_data = torch.tensor(test_data)\n",
        "    test_labels = torch.tensor(test_labels)\n",
        "\n",
        "    xtest = torch.tensor(test_data).permute(0, 3, 1, 2)\n",
        "    testset = linkDataset(xtest, test_labels)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=bs, drop_last=True, shuffle=True) \n",
        "    print('test data:', xtest.shape)\n",
        "    \n",
        "    Params['Img_size'] = img_size\n",
        "    Params[\"c_dim\"] = c_dim\n",
        "\n",
        "    return train_loader, val_loader, test_loader, Params\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEnrjUmT3k1b"
      },
      "source": [
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, Params, best_model_stats):\n",
        "\n",
        "    num_epochs = Params['num_epochs']\n",
        "\n",
        "    train_loss_history = []\n",
        "    val_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      # TRAINING PHASE \n",
        "      model.train()\n",
        "      train_loss = torch.tensor(0., device=device)\n",
        "      train_acc = torch.tensor(0., device='cpu')\n",
        "\n",
        "      for i, Xy in enumerate(train_dataloader):\n",
        "        X = Xy[0].float()\n",
        "        X = resize_images(X, 256).to(device)\n",
        "        y = Xy[1].to(device)\n",
        "        y = one_hot_embedding(y, 45).float().to(device)\n",
        "\n",
        "        # zero the learnable weight gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        yhat = model(X)\n",
        "        yhat = torch.squeeze(yhat)\n",
        "        loss = criterion(yhat, y)\n",
        "        acc = (yhat.argmax(dim=1) == y.argmax(dim=1)).float().mean()\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # update loss sum\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc.item()\n",
        "\n",
        "      # compute average training loss\n",
        "      train_loss = (train_loss/(i+1)).data.item()\n",
        "      train_loss_history.append(train_loss)\n",
        "\n",
        "      train_acc = (train_acc/(i+1)).data.item()\n",
        "      train_acc_history.append(train_acc)\n",
        "      \n",
        "      # VALIDATION  PHASE \n",
        "      model.eval()\n",
        "      val_loss = 0.0\n",
        "      val_acc = 0.0\n",
        "\n",
        "      for i, Xy in enumerate(val_dataloader):\n",
        "        X = Xy[0].float()\n",
        "        X = resize_images(X, 256).to(device)\n",
        "        y = Xy[1].to(device)\n",
        "        y = one_hot_embedding(y, 45).float().to(device)\n",
        "\n",
        "        # get outputs and compute loss\n",
        "        yhat = model(X)\n",
        "        yhat = torch.squeeze(yhat)\n",
        "        loss = criterion(yhat, y)\n",
        "        acc = (yhat.argmax(dim=1) == y.argmax(dim=1)).float().mean()\n",
        "\n",
        "        # update loss sum\n",
        "        val_loss += loss.item()\n",
        "        val_acc += acc.item()\n",
        "\n",
        "      # compute average training loss\n",
        "      val_loss = val_loss/(i+1)\n",
        "      val_loss_history.append(val_loss)\n",
        "\n",
        "      val_acc = val_acc/(i+1)\n",
        "      val_acc_history.append(val_acc)\n",
        "\n",
        "      # save best model \n",
        "      if best_model_stats:\n",
        "        if (val_loss < best_model_stats['val_loss']):\n",
        "          if Params['save_model']:\n",
        "            torch.save(model, 'best_CNN_model.pt')  # save entire model\n",
        "          best_model_stats['val_acc'] = val_acc\n",
        "          best_model_stats['val_loss'] = val_loss\n",
        "          best_model_flag = \"* Best Model\"  # highlight in output\n",
        "        else:\n",
        "          best_model_flag = \"\"  # clear highlight in output\n",
        "      else: # first time\n",
        "        if Params['save_model']:\n",
        "          torch.save(model, 'best_CNN_model.pt')  # save entire model    \n",
        "        best_model_stats['val_acc'] = val_acc\n",
        "        best_model_stats['val_loss'] = val_loss\n",
        "        best_model_flag = \"\"\n",
        "\n",
        "      # print running results\n",
        "      if Params['verbose']:\n",
        "        if (epoch % Params['output_int'] == 0):\n",
        "          print(\"     epoch: {:d} |  runtime(m): {:.1f}  | train_loss: {:2.3f} | val_loss: {:2.3f} | train_acc: {:2.2f} | val_acc: {:2.2f}  {}\".\n",
        "                format(epoch+1, (time.time()-t0)/60, train_loss, val_loss, train_acc, val_acc,  best_model_flag))\n",
        "\n",
        "    history = {'epoch': range(1,num_epochs+1),\n",
        "               'train_loss': train_loss_history,\n",
        "               'val_loss': val_loss_history,\n",
        "               'train_acc': train_acc_history,\n",
        "               'val_acc': val_acc_history}\n",
        "\n",
        "    return ([history, best_model_stats])\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LJdfsa89eBO",
        "outputId": "dc1bf893-d01c-45b6-debb-60aa24707058"
      },
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    Params = {'datasize':96, \n",
        "              'in_channels':3, \n",
        "              'num_classes':45, \n",
        "              'test_pct':0.25,  # testset split percentage\n",
        "              'dataset_pct':1,  # percent data to use. set smaller for faster testing. 1 for full dataset.\n",
        "              'aug_pct':0.0,    # percent of training data to increase with augmented images. 0 for no augmentation.  \n",
        "              'aug_list':[1,1,0,0,0,0,0],  # toggle augmentations to apply if augmenting training data. (is to hflip,vflip only)\n",
        "              'batch_size':512, \n",
        "              'num_epochs':600, \n",
        "              'lr':0.00001, \n",
        "              'verbose':True, \n",
        "              'output_int':1, \n",
        "              'save_model':True} \n",
        "\n",
        "    model = Conv_Pred()\n",
        "    model.weight_init()\n",
        "    model.to(device)\n",
        "\n",
        "    bce_loss = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = Params['lr'])\n",
        "\n",
        "    # create train/test datafiles if necessary\n",
        "    # currently loading image data saved at 96x96 resolution\n",
        "    load_data_files(Params['test_pct']) \n",
        "    \n",
        "    train_loader, val_loader, test_loader, Params = get_data(Params, load=True)\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image data: (31500, 96, 96, 3)\n",
            "img_size: 96\n",
            "train data: torch.Size([21261, 3, 96, 96])\n",
            "val data: torch.Size([5907, 3, 96, 96])\n",
            "test data: torch.Size([7875, 3, 96, 96])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIATXob4_ViU"
      },
      "source": [
        " \n",
        "# train model and return performance over epochs\n",
        "history, best = train_model(model, train_loader, val_loader, optimizer, bce_loss, Params, best_model_stats={})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yON7SS2OBv1r"
      },
      "source": [
        "\n",
        "# plot results\n",
        "import seaborn as sns\n",
        "\n",
        "sns.lineplot(data=history, x='epoch', y='train_loss', label='train_loss', color=\"blue\")\n",
        "sns.lineplot(data=history, x='epoch', y='val_loss', label='val_loss', color=\"red\")\n",
        "plt.legend()\n",
        "plt.title(\"lr: \" + str(Params['lr']))\n",
        "plt.show()\n",
        "\n",
        "sns.lineplot(data=history, x='epoch', y='train_acc', label='train_acc', color=\"blue\")\n",
        "sns.lineplot(data=history, x='epoch', y='val_acc', label='val_acc', color=\"red\")\n",
        "plt.legend()\n",
        "plt.title(\"lr: \" + str(Params['lr']))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bilZjXqXLKGi"
      },
      "source": [
        "\n",
        "# hyperparameter tuning\n",
        "\n",
        "lr_list = [0.0001, 0.00005, 0.00001]\n",
        "\n",
        "all_history_df = pd.DataFrame()\n",
        "best_model_stats = {}\n",
        "\n",
        "for i,lr in enumerate(lr_list):\n",
        "\n",
        "    Params = {'num_epochs':500, \n",
        "              'lr':lr, \n",
        "              'verbose':True, \n",
        "              'output_int':20, \n",
        "              'save_model':False} \n",
        "\n",
        "    try:\n",
        "      del model\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "    model = Conv_Pred()\n",
        "    model.weight_init()\n",
        "    model.to(device)\n",
        "\n",
        "    bce_loss = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = Params['lr'])\n",
        "\n",
        "    print(\"\\n grid search iteration {} of {} | num_epochs: {:d} | learning_rate: {:1.8f}\\n\".\n",
        "          format(i+1, len(lr_list), Params['num_epochs'], lr))\n",
        "     \n",
        "    # train model and return performance over epochs\n",
        "    history, best_model_stats = train_model(model, train_loader, val_loader, \n",
        "                                            optimizer, bce_loss, Params, best_model_stats)\n",
        "\n",
        "    history['lr'] = lr\n",
        "    history['num_epochs'] = Params['num_epochs']\n",
        "         \n",
        "    all_history_df = all_history_df.append(pd.DataFrame(history))\n",
        "    all_history_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    best_idx = all_history_df['val_loss'].argmin()\n",
        "    print(\"\\n best current model:  lr: {:1.7f} | epoch: {:d} | val_loss: {:1.3f} | val_acc: {:1.3f}\\n\".format(\n",
        "        all_history_df.loc[best_idx, 'lr'], \n",
        "        all_history_df.loc[best_idx, 'epoch'], \n",
        "        all_history_df.loc[best_idx, 'val_loss'], \n",
        "        all_history_df.loc[best_idx, 'val_acc']))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zDsSEp-WGRN"
      },
      "source": [
        "# plot grid search results\n",
        "\n",
        "sns.lineplot(data=all_history_df, x='epoch', y='val_loss', hue=all_history_df['lr'].astype('str'))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "sns.lineplot(data=all_history_df, x='epoch', y='val_acc', hue=all_history_df['lr'].astype('str'))\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}