{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS541_Project_MAML_and_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3M5AQyC7Wak"
      },
      "source": [
        "#QUINCY HERSHEY - ALEX MOORE - ADAM DICHIARA - SCOTT TANG - VINCENT FILARDI\n",
        "#CS541 Project: MAML CNN\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sENfvlY5AUU"
      },
      "source": [
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "colab=True\n",
        "if colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "dir = '/content/drive/MyDrive/Project/data/'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qydxx8m5Nreh"
      },
      "source": [
        "## RESISC45\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILyzj825ovVU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfMUUIpq5Ps6"
      },
      "source": [
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "test_size = 0.8\n",
        "val_size = 0.2\n",
        "batch_size = 32\n",
        "MAML_learning_rate1 = 0.0006\n",
        "MAML_learning_rate2 = 0.0006\n",
        "CNN_learning_rate = 0.0006\n",
        "MAML2_learning_rate1=.00045\n",
        "MAML2_learning_rate2=25e-05\n",
        "n_epochs = 150\n",
        "print_stride = 10\n",
        "Freeze=False\n",
        "First_Train = True\n",
        "no_load = True\n",
        "bce_loss = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CAjypAqQYwU"
      },
      "source": [
        "class CustomTensorDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, labels, transform=None):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform=transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.images[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "def make_dataset(x, y):\n",
        "    x = torch.tensor(x).permute(0,3,1,2)\n",
        "    y = torch.tensor(y)\n",
        "    dset = CustomTensorDataset(x, y)\n",
        "    return dset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "240INoPLKeOx"
      },
      "source": [
        "def get_data(dir, mtype):\n",
        "    if mtype=='MAML': batch_n = batch_size*2\n",
        "    else: batch_n = batch_size\n",
        "\n",
        "    data = np.load(dir+'RESISC45_images_96.npy')\n",
        "    labels = np.load(dir+'RESISC45_classes.npy')\n",
        "    classes = np.load(dir+'RESISC45_class_names.npy')\n",
        "\n",
        "    #img_size = train_data.shape[2]\n",
        "    c_dim = classes.shape[0]\n",
        "\n",
        "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size = test_size, stratify = labels)\n",
        "\n",
        "    xtrain, xval, ytrain, yval = train_test_split(train_data, train_labels, test_size = val_size)\n",
        "\n",
        "    trainset = make_dataset(xtrain,ytrain)\n",
        "    train_loader = DataLoader(trainset, batch_size=int(batch_n), shuffle=True)\n",
        "\n",
        "    valset = make_dataset(xval, yval)\n",
        "    val_loader = torch.utils.data.DataLoader(valset, batch_n, drop_last = True, shuffle=True)\n",
        "\n",
        "    testset = make_dataset(test_data, test_labels)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_n, drop_last = True, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, c_dim, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euJKRHjg62sI"
      },
      "source": [
        "class Conv_Pred(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Conv_Pred, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 32, 4, 2, 1) # Input: (batch_size, 3, img_size, img_size)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 4, 2, 1, bias = False)\n",
        "        self.conv2_bn = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 4, 2, 1, bias = False)\n",
        "        self.conv3_bn = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4, 2, 1, bias = False)\n",
        "        self.conv4_bn = nn.BatchNorm2d(256)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 4, 2, 1, bias = False)\n",
        "        self.conv5_bn = nn.BatchNorm2d(512)\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 4, 2, 1, bias = False)\n",
        "        self.conv6_bn = nn.BatchNorm2d(1024)\n",
        "        \n",
        "        self.fce = nn.Linear(1024, 45)\n",
        "\n",
        "\n",
        "    def weight_init(self):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x/127.5 - 1.0\n",
        "        x = F.relu(self.conv1(x), 0.2)\n",
        "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
        "        x = F.relu(self.conv3_bn(self.conv3(x)))\n",
        "        x = F.relu(self.conv4_bn(self.conv4(x)))\n",
        "        x = F.relu(self.conv5_bn(self.conv5(x)))\n",
        "        x = F.relu(self.conv6_bn(self.conv6(x)))\n",
        "\n",
        "        z = nn.Softmax(dim=1)(self.fce(x.squeeze()))\n",
        "\n",
        "        return z\n",
        "\n",
        "    def save(self, name):\n",
        "        print('saving...')\n",
        "        torch.save({\n",
        "            'model_state_dict': self.state_dict() #,\n",
        "            #'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "        }, dir+name+'.pt')\n",
        "\n",
        "    def load(self, name):\n",
        "        print('loading...')\n",
        "        checkpoint = torch.load(dir+name+'.pt')\n",
        "        self.load_state_dict(checkpoint['model_state_dict'])\n",
        "        #self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "def get_model(mtype, lr=0.0006, train=True,load=False):\n",
        "    model = Conv_Pred().to(device)\n",
        "    model.weight_init()\n",
        "    if load==True:\n",
        "        try: model.load(mtype)\n",
        "        except: pass\n",
        "    if train == True:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def normal_init(m):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "\n",
        "def one_hot_embedding(labels, dims):\n",
        "    labels = torch.nn.functional.one_hot(torch.tensor(labels).to(torch.int64), num_classes = dims)\n",
        "    return torch.squeeze(labels)\n",
        "\n",
        "def accuracy_topk(output, target, topk=(3,)):\n",
        "    #https://forums.fast.ai/t/return-top-k-accuracy/27658\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0JZD2K589Mw"
      },
      "source": [
        "#augmentation_functions.py\n",
        "\n",
        "class addGaussianNoise(object):\n",
        "    def __init__(self, mean=0.0, std=1.0, p=0.5):\n",
        "        self.mean = torch.tensor(mean).to(device)\n",
        "        self.std = torch.tensor(std).to(device)\n",
        "        self.p = p\n",
        "      \n",
        "    def __call__(self, img):\n",
        "        if torch.rand(1).item() < self.p:\n",
        "            return img + torch.randn(img.size(), device = device) * self.std + self.mean\n",
        "        return img\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1}, p={2})'.format(self.mean, self.std, self.p)\n",
        "\n",
        "def applyAugs(img_batch, task_idx, num_augs=7):\n",
        "    # returns augmented batch of images based on task index (0:128)\n",
        "    # currently based on exactly 7 transforms \n",
        "\n",
        "    transform_list = [transforms.RandomHorizontalFlip(p=0.99),\n",
        "                      transforms.RandomVerticalFlip(p=0.99),\n",
        "                      transforms.RandomRotation(359.0, fill=0.5),\n",
        "                      transforms.RandomPerspective(distortion_scale=0.1, p=0.99, fill=0.5),\n",
        "                      transforms.RandomResizedCrop(96,\n",
        "                                                   scale=(0.5, 1.0),\n",
        "                                                   ratio=(0.8, 1.0)),\n",
        "                                                   #interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                      addGaussianNoise(std=0.1, p=0.99),\n",
        "                      # transforms.ColorJitter(saturation=4.0, hue=0.01),\n",
        "                      transforms.ColorJitter(brightness=0.5, contrast=0.9)\n",
        "                      # ,transforms.GaussianBlur(9, sigma=(0.01, 2.0))\n",
        "                      ]\n",
        "       \n",
        "    tasklist = list(itertools.product([0, 1], repeat=num_augs))\n",
        "    current_augs = tasklist[task_idx]\n",
        "\n",
        "    task_transforms = [transform_list[i] for i,x in enumerate(current_augs) if x==1]\n",
        "    transform = torchvision.transforms.Compose(task_transforms)\n",
        "    img_batch = transform(img_batch)\n",
        "    return img_batch\n",
        "\n",
        "def getAugmentationTransforms(task_idx, num_augs=7):\n",
        "    # returns transforms.Compose function of transforms based on task index (0:128)\n",
        "    # currently based on exactly 7 transforms \n",
        "\n",
        "    transform_list = [transforms.RandomHorizontalFlip(p=0.99),\n",
        "                      transforms.RandomVerticalFlip(p=0.99),\n",
        "                      transforms.RandomRotation(359.0, fill=0.5),\n",
        "                      transforms.RandomPerspective(distortion_scale=0.1, p=0.99, fill=0.5),\n",
        "                      transforms.RandomResizedCrop(256,\n",
        "                                                   scale=(0.5, 1.0),\n",
        "                                                   ratio=(1.0, 1.0),\n",
        "                                                   interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "                      addGaussianNoise(std=0.1, p=0.99),\n",
        "                      # transforms.ColorJitter(saturation=4.0, hue=0.01),\n",
        "                      transforms.ColorJitter(brightness=0.5, contrast=0.9)\n",
        "                      # ,transforms.GaussianBlur(9, sigma=(0.01, 2.0))\n",
        "                      ]\n",
        "       \n",
        "    tasklist = list(itertools.product([0, 1], repeat=num_augs))\n",
        "    current_augs = tasklist[task_idx]\n",
        "\n",
        "    task_transforms = [transform_list[i] for i,x in enumerate(current_augs) if x==1]\n",
        "    transform = torchvision.transforms.Compose(task_transforms)\n",
        "\n",
        "    return transform\n",
        "\n",
        "# utility functions\n",
        "# images must be normalized and converted to torch shape before augmentations (3,h,w)\n",
        "# converted to numpy shape for displaying (h,w,3)\n",
        "\n",
        "def normalizeImages(x):\n",
        "  x = x/255.\n",
        "  return x\n",
        "\n",
        "def convertToTorch(x):\n",
        "  x = np.moveaxis(x, 3, 1)\n",
        "  x = torch.as_tensor(x)\n",
        "  return x\n",
        "\n",
        "def convertToNumpy(x):\n",
        "  # convert back to format for displaying\n",
        "  x = x.numpy()\n",
        "  x = np.moveaxis(x, 1, 3)  \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ykjpav_7GRG"
      },
      "source": [
        "def train(n_epochs, learning_rate, learning_rate1, train_loader, val_loader, c_dim, mtype, augs=True, print_stride1=print_stride):\n",
        "    epoch_tracker, loss_tracker, val_accs, val_topks = [], [], [], []\n",
        "    i = 0\n",
        "    tasks = 128\n",
        "\n",
        "    model = get_model(mtype=mtype)\n",
        "    model_optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        for X, y in train_loader:\n",
        "\n",
        "            y = one_hot_embedding(y, c_dim).float().to(device)\n",
        "\n",
        "            if mtype=='MAML':\n",
        "\n",
        "                X1 = X[:int(X.shape[0]/2)].float().clone().to(device)\n",
        "                X2 = X[int(X.shape[0]/2):].float().clone().to(device)\n",
        "                y1 = y[:int(y.shape[0]/2)].float().clone().to(device)\n",
        "                y2 = y[int(y.shape[0]/2):].float().clone().to(device)\n",
        "\n",
        "                task_batch1 = applyAugs(X1, int(i%tasks)).to(device)\n",
        "                task_batch2 = applyAugs(X2, int(i%tasks)).to(device)\n",
        "                i += 1\n",
        "\n",
        "                model1 = copy.deepcopy(model).to(device)\n",
        "                model1_optimizer = optim.Adam(model1.parameters(), lr = learning_rate1)\n",
        "\n",
        "                for param in model.parameters(): param.grad = None\n",
        "                for param in model1.parameters(): param.grad = None\n",
        "\n",
        "                yhat = model1(task_batch1)\n",
        "                pred_loss = bce_loss(yhat, y1)\n",
        "                pred_loss.backward()\n",
        "\n",
        "                model1_optimizer.step()\n",
        "\n",
        "                losses += [pred_loss.item()]\n",
        "                for param in model.parameters(): param.grad = None\n",
        "                for param in model1.parameters(): param.grad = None\n",
        "\n",
        "                yhat = model1(task_batch2)\n",
        "                pred_loss = bce_loss(yhat, y2)\n",
        "                pred_loss.backward()\n",
        "\n",
        "                for m1, m2 in zip(model.named_parameters(), model1.named_parameters()):\n",
        "                    m1[1].grad = m2[1].grad.clone()\n",
        "            else: \n",
        "              \n",
        "                X = X.float().clone().to(device)\n",
        "                t = np.random.randint(tasks)\n",
        "\n",
        "                if augs: X = applyAugs(X, t).to(device)\n",
        "\n",
        "                for param in model.parameters(): param.grad = None\n",
        "\n",
        "                yhat = model(X)\n",
        "                pred_loss = bce_loss(yhat, y)\n",
        "                pred_loss.backward()\n",
        "\n",
        "            model_optimizer.step()\n",
        "\n",
        "            losses += [pred_loss.item()]\n",
        "            \n",
        "        \n",
        "        if epoch % print_stride1 == 0:\n",
        "            print('Epoch {} - loss: {:.3f}'.format((epoch), torch.mean(torch.FloatTensor(losses))))\n",
        "\n",
        "            loss_tracker.append(torch.mean(torch.FloatTensor(losses)))\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                accs, actk = [], []\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(device).float(), y.to(device).float()\n",
        "                    yhat = model(x)\n",
        "                    \n",
        "                    yhat_max = torch.max(yhat, dim = 1)[1]\n",
        "                    \n",
        "                    correct = torch.sum(yhat_max == y)\n",
        "                    size = x.shape[0]\n",
        "                    \n",
        "                    acc_topk = accuracy_topk(yhat, y)\n",
        "                    actk.append(acc_topk.data.item())\n",
        "                    \n",
        "                    accs.append(100*(correct/size).data.item())\n",
        "\n",
        "                print('Validation Accuracy: ', torch.mean(torch.FloatTensor(accs)).data.item())\n",
        "                print('Validation Top3 Accuracy: ', torch.mean(torch.FloatTensor(actk)).data.item())\n",
        "                \n",
        "            val_accs.append(torch.mean(torch.FloatTensor(accs)))\n",
        "            val_topks.append(torch.mean(torch.FloatTensor(actk)))\n",
        "            epoch_tracker.append(epoch)\n",
        "          \n",
        "    np.save(dir+mtype+'_loss_tracker.npy', loss_tracker) \n",
        "    np.save(dir+mtype+'_val_topks.npy', val_topks)\n",
        "    np.save(dir+mtype+'_val_accs.npy', val_accs)\n",
        "    np.save(dir+'epoch_tracker.npy', epoch_tracker)\n",
        "    model.save(mtype)\n",
        "\n",
        "    return model, loss_tracker, val_topks, val_accs, epoch_tracker\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRED1GMzkM_7"
      },
      "source": [
        "## MORE MMAML FEATURES N TASK BATCH SIZE AND REGULARIZIAING INNER LOOP TO MIDIGATE FIRST ORDER APROXIMATION\n",
        "def train2(n_epochs, learning_rate, learning_rate1, train_loader, val_loader, c_dim, mtype,print_stride1 = print_stride):\n",
        "    epoch_tracker, loss_tracker, val_accs, val_topks = [], [], [], []\n",
        "    i = 0\n",
        "    tasks = 128\n",
        "    alpha=.08\n",
        "\n",
        "    model = get_model(mtype=mtype)\n",
        "    model_optimizer = optim.Adam(model.parameters(), lr = learning_rate,weight_decay=0)\n",
        "    n=32\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "  \n",
        "        losses = []\n",
        "        j=0\n",
        "        TasksInOuterSum = n\n",
        "        pred_loss=torch.tensor(0.)\n",
        "        model3=copy.deepcopy(model).to(device)## we will store all the grads here in this model it will act as a container\n",
        "        for param in model3.parameters(): param.grad = torch.zeros_like(param,dtype=torch.float, requires_grad=True,device=device).float() ## set grads equal to o\n",
        "\n",
        "        for X, y in train_loader:\n",
        "            i = torch.randint(high=127, size=(1, 1)).item()\n",
        "            y = one_hot_embedding(y, c_dim).float().to(device)\n",
        "\n",
        "            if mtype=='MAML':\n",
        "                ## I added support for having the outer loop contain multiple tasks, it has been cited by Cheasea finn, \n",
        "                #the creator of MAML for fast adaption this is where the model does the most work the outerloop, so we will back load it with 256 images instead of 32\n",
        "                # This will be done by doing the update of the j variable is a multiple of n, for instance if n is = 8 in the code see line  so we get 8*32=256 images all form same tasks as the inner loop per outer update\n",
        "                X = applyAugs(X.float().clone().to(device), int(i))## I put the aug here to make sure that outer and inner loop train on same task as in the MAML paper\n",
        "\n",
        "                ## data prep for inner and outer loop\n",
        "                task_batch1 = X[:int(X.shape[0]/2)].float().clone().to(device)\n",
        "                task_batch2 = X[int(X.shape[0]/2):].float().clone().to(device) ## now each batch\n",
        "                y1 = y[:int(y.shape[0]/2)].float().clone().to(device)\n",
        "                y2 = y[int(y.shape[0]/2):].float().clone().to(device)\n",
        "                ## start inner loop\n",
        "                ## with first order we update the models weights after each inner loop to prevent the hessian update\n",
        "                ## We wish to find a good intiiallzation point for all tasks so this inner loop should either have a small learning rate or a regularizer\n",
        "                ## I will use a regulizer to enforce some closeness to the initial point, note te more images we have in the outer loop update step the further we will be \n",
        "                ## from the initialization point, ideally maml without any apoximation calls aims to acheive this\n",
        "                \n",
        "                ## get logits with the loss for the inner loop\n",
        "                model1 = copy.deepcopy(model).to(device)\n",
        "                model1_optimizer = optim.Adam(model1.parameters(), lr = learning_rate1,weight_decay =alpha)\n",
        "\n",
        "                for param in model.parameters(): param.grad = None\n",
        "                for param in model1.parameters(): param.grad = None\n",
        "\n",
        "                inner_logit = model1(task_batch1)\n",
        "                inner_loss = bce_loss(inner_logit, y1)\n",
        "                inner_loss.backward() ## innner backwards\n",
        "                model1_optimizer.step() ## inner loss update not on orginal model\n",
        "                losses += [inner_loss.item()]\n",
        "\n",
        "                for param in model.parameters(): param.grad = None\n",
        "                for param in model1.parameters(): param.grad = None\n",
        "\n",
        "                ## now with the updated_params dict we will take outerloop without updating the params on the inner loop\n",
        "                outer_logit =  model1(task_batch2) ## get outer logits  with inner loss's params\n",
        "                outer_loss = bce_loss(outer_logit,y2)\n",
        "                outer_loss.backward()\n",
        "\n",
        "                for m1, m2 in zip(model3.named_parameters(), model1.named_parameters()): ## add weights to container\n",
        "                    m1[1].grad = m1[1].grad + m2[1].grad.clone()\n",
        "\n",
        "                ## we will now update the model after seeing our n tasks with j%n\n",
        "                ## time to clone\n",
        "                if j%TasksInOuterSum ==0:\n",
        "                    for m1, m2 in zip(model.named_parameters(), model3.named_parameters()):\n",
        "                        m1[1].grad = m2[1].grad.clone()\n",
        "                    ## do the update here\n",
        "                    model_optimizer.step()\n",
        "                    for param in model.parameters(): param.grad = None\n",
        "                    for param in model1.parameters(): param.grad = None\n",
        "                    for param in model3.parameters(): param.grad = torch.zeros_like(param,dtype=torch.float, requires_grad=True,device=device).float() ## set grads equal to o\n",
        "                losses += [outer_loss.item()]\n",
        "    \n",
        "            else: \n",
        "              \n",
        "                X = X.float().clone().to(device)\n",
        "                t = np.random.randint(tasks)\n",
        "                task_batch = applyAugs(X, t).to(device)\n",
        "\n",
        "                for param in model.parameters(): param.grad = None\n",
        "\n",
        "                yhat = model(task_batch)\n",
        "                pred_loss = bce_loss(yhat, y)\n",
        "                pred_loss.backward()\n",
        "                ## moved these inside, We only wish to do it for the cnn\n",
        "                model_optimizer.step()\n",
        "            losses += [pred_loss.item()]\n",
        "            \n",
        "        \n",
        "        if epoch % print_stride1 == 0:\n",
        "\n",
        "            print('Epoch {} - loss: {:.3f}'.format((epoch), torch.mean(torch.FloatTensor(losses))))\n",
        "\n",
        "            loss_tracker.append(torch.mean(torch.FloatTensor(losses)))\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                accs, actk = [], []\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(device).float(), y.to(device).float()\n",
        "                    yhat = model(x)\n",
        "                    \n",
        "                    yhat_max = torch.max(yhat, dim = 1)[1]\n",
        "                    \n",
        "                    correct = torch.sum(yhat_max == y)\n",
        "                    size = x.shape[0]\n",
        "\n",
        "                    acc_topk = accuracy_topk(yhat, y)\n",
        "                    actk.append(acc_topk.data.item())\n",
        "                    \n",
        "                    accs.append(100*(correct/size).data.item())\n",
        "\n",
        "            print('Validation Accuracy: ', torch.mean(torch.FloatTensor(accs)).data.item())\n",
        "            print('Validation Top3 Accuracy: ', torch.mean(torch.FloatTensor(actk)).data.item())\n",
        "            val_accs.append(torch.mean(torch.FloatTensor(accs)))\n",
        "            val_topks.append(torch.mean(torch.FloatTensor(actk)))\n",
        "            epoch_tracker.append(epoch)\n",
        "    np.save(dir+mtype+'_loss_tracker.npy', loss_tracker) \n",
        "    np.save(dir+mtype+'_val_topks.npy', val_topks)\n",
        "    np.save(dir+mtype+'_val_accs.npy', val_accs)\n",
        "    np.save(dir+'epoch_tracker.npy', epoch_tracker)\n",
        "    model.save(mtype)\n",
        "\n",
        "    return model, loss_tracker, val_topks, val_accs, epoch_tracker\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXe2IDIA6wws"
      },
      "source": [
        "def make_train_plot(MAML_loss_tracker, CNN_Aug_loss_tracker, CNN_loss_tracker, MAML_val_accs, CNN_Aug_val_accs, CNN_val_accs, MAML_val_topks, CNN_Aug_val_topks, CNN_val_topks, epoch_tracker,MAML_loss_tracker2,MAML_val_accs2,MAML_val_topks2):\n",
        "    #plt.plot(epoch_tracker, MAML_loss_tracker, label = 'MAML train loss')\n",
        "    #plt.plot(epoch_tracker, CNN_Aug_loss_tracker, label = 'CNN Aug train loss')\n",
        "    #plt.plot(epoch_tracker, CNN_loss_tracker, label = 'CNN train loss')\n",
        "    plt.figure(figsize = (15,8))\n",
        "    plt.plot(epoch_tracker, MAML_val_accs, label = 'MAML val acc', color='green', linestyle='dashed')\n",
        "    plt.plot(epoch_tracker, MAML_val_accs2, label = 'MAML_ val acc', color='orange', linestyle='dashed')\n",
        "\n",
        "    plt.plot(epoch_tracker, CNN_Aug_val_accs, label = 'CNN Aug val acc', color='blue', linestyle='dashed')\n",
        "    plt.plot(epoch_tracker, CNN_val_accs, label = 'CNN val acc', color='red', linestyle='dashed')\n",
        "    plt.plot(epoch_tracker, MAML_val_topks, label = 'MAML val top3', color='green')\n",
        "    plt.plot(epoch_tracker, MAML_val_topks2, label = 'MAML_ val top3', color='orange')\n",
        "\n",
        "    plt.plot(epoch_tracker, CNN_Aug_val_topks, label = 'CNN_Aug_val top3', color='blue')\n",
        "    plt.plot(epoch_tracker, CNN_val_topks, label = 'CNN_val top3', color='red')\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.show()\n",
        "\n",
        "def make_two_plots(epoch_tracker, MAML_loss_tracker, CNN_Aug_loss_tracker, CNN_loss_tracker, MAML_val_accs, CNN_Aug_val_accs, CNN_val_accs,MAML_loss_tracker2,MAML_val_accs2,MAML_val_topks2):\n",
        "    fig, (ax1, ax2) = plt.subplots(1,2)\n",
        "    fig.set_size_inches(15,8)\n",
        "    ax1.plot(epoch_tracker, MAML_loss_tracker, label = 'MAML train loss', color='green')\n",
        "    ax1.plot(epoch_tracker, MAML_loss_tracker2, label = 'MAML_ train loss', color='orange')\n",
        "\n",
        "    ax1.plot(epoch_tracker, CNN_Aug_loss_tracker, label = 'CNN Aug train loss', color='blue')\n",
        "    ax1.plot(epoch_tracker, CNN_loss_tracker, label = 'CNN train loss', color='red')\n",
        "    ax1.legend(loc = 'best')\n",
        "    ax2.plot(epoch_tracker, MAML_val_accs, label = 'MAML val acc', color='green')\n",
        "    ax2.plot(epoch_tracker, MAML_val_accs2, label = 'MAML_ val acc', color='orange')\n",
        "\n",
        "    ax2.plot(epoch_tracker, CNN_Aug_val_accs, label = 'CNN Aug val acc', color='blue')\n",
        "    ax2.plot(epoch_tracker, CNN_val_accs, label = 'CNN val acc', color='red')\n",
        "    ax2.legend(loc = 'best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def basic_train_plot(epoch_tracker, MAML_loss_tracker, CNN_Aug_loss_tracker, CNN_loss_tracker,MAML_loss_tracker2):\n",
        "    plt.figure(figsize = (15,8))\n",
        "    plt.plot(epoch_tracker, MAML_loss_tracker, label = 'MAML train loss', color='green')\n",
        "    plt.plot(epoch_tracker, MAML_loss_tracker2, label = 'MAML_ train loss', color='orange')\n",
        "\n",
        "    plt.plot(epoch_tracker, CNN_Aug_loss_tracker, label = 'CNN Aug train loss', color='blue')\n",
        "    plt.plot(epoch_tracker, CNN_loss_tracker, label = 'CNN train loss', color='red')\n",
        "    plt.legend(loc = 'best')\n",
        "    plt.show()\n",
        "\n",
        "def make_test(model, testloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        accs, topks, ypreds, yreals = [], [], [], []\n",
        "        for x, y in testloader:\n",
        "            x, y = x.to(device).float(), y.to(device).float()\n",
        "            yhat = model(x)\n",
        "\n",
        "            yhat_max = torch.max(yhat, dim = 1)[1]\n",
        "\n",
        "            correct = torch.sum(yhat_max == y)\n",
        "            size = x.shape[0]\n",
        "            acc_topk = accuracy_topk(yhat, y)\n",
        "\n",
        "            ypreds += torch.argmax(yhat, dim=1).tolist()\n",
        "            yreals += y.tolist()\n",
        "\n",
        "            topks.append(acc_topk.data.item())\n",
        "            accs.append(100*(correct/size).data.item())\n",
        "\n",
        "        print('Test Accuracy: ', torch.mean(torch.FloatTensor(accs)).data.item())\n",
        "        print('Test Top3 Accuracy: ', torch.mean(torch.FloatTensor(topks)).data.item())\n",
        "\n",
        "        cm = metrics.confusion_matrix(yreals, ypreds)\n",
        "        fig, ax = plt.subplots(figsize=(15,8))  \n",
        "        sns.heatmap(cm, cmap='Blues', ax=ax)\n",
        "        ## get best/worst task\n",
        "        model.eval()\n",
        "        worst,best = get_worst_task(model,testloader)\n",
        "        print(best,'= (acc percent correct per task,taskID) for best ')\n",
        "        print(worst,'= (acc percent correct per,taskID) for worst ')\n",
        "        return(torch.mean(torch.FloatTensor(accs)).data.item()),torch.mean(torch.FloatTensor(topks)).data.item()))\n",
        "        \n",
        "def get_worst_task(model,val_loader): ## gives percentage rate of correct classification for each aug in our task list, \n",
        "                                      ##please check if dict on line below is right size\n",
        "    bytaskListACCDICT = dict.fromkeys(range(128),0)\n",
        "    i=0\n",
        "    model.eval()\n",
        "    for x,y in val_loader:\n",
        "        actk,accs = [], []\n",
        "        x = applyAugs(x.float().clone().to(device), int(i%128))## I put the aug here to make sure that outer and inner loop train on same task as in the MAML paper\n",
        "        y = y.float().clone().to(device)\n",
        "        yhat = model(x) \n",
        "        yhat_max = torch.max(yhat, dim = 1)[1]\n",
        "        correct = torch.sum(yhat_max == y)\n",
        "        size = x.shape[0]\n",
        "        acc_topk = accuracy_topk(yhat, y)\n",
        "        actk.append(acc_topk.data.item())\n",
        "        \n",
        "        accs.append(100*(correct/size).data.item())\n",
        "        bytaskListACCDICT[i%128]+=100*(correct/size).data.item()\n",
        "        i+=1\n",
        "    worstTASKid = min(bytaskListACCDICT, key=bytaskListACCDICT.get)\n",
        "    bestTASKid = max(bytaskListACCDICT, key=bytaskListACCDICT.get)\n",
        "\n",
        "    return (bytaskListACCDICT[worstTASKid]/math.floor((len(val_loader)/128)),worstTASKid), (bytaskListACCDICT[bestTASKid]/math.floor((len(val_loader)/128)),bestTASKid)\n",
        "\n",
        "def make_example(train_loader, classes):\n",
        "    #print('Shape of a batch of images: ', next(iter(train_loader))[0].shape)\n",
        "    #print('Shape of a batch of labels: ', next(iter(train_loader))[1].shape)\n",
        "\n",
        "    plt.figure(figsize = (8,8))\n",
        "    first_samp = next(iter(train_loader)) #get first sample in first batch\n",
        "    img, name = first_samp[0][0], first_samp[1][0]\n",
        "    plt.imshow(img.permute(1,2,0)/255)\n",
        "\n",
        "    print(name, classes[name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gftKXCpdiY6"
      },
      "source": [
        "## RESISC MAML\n",
        "\n",
        "Require: $p(\\mathcal{T}):$ distribution over tasks \n",
        "Require: $\\alpha, \\beta:$ step size hyperparameters\n",
        "\n",
        "1: randomly initialize $\\theta$\n",
        "\n",
        "2: while not done $\\mathbf{d o}$ \n",
        "\n",
        "3: $\\quad$ Sample single task $\\mathcal{T} \\sim p(\\mathcal{T})$\n",
        "\n",
        "5: $\\quad$ Evaluate $\\nabla_{\\theta} \\mathcal{L}\\left(f_{\\theta}(\\mathcal{T}_{inner})\\right)$ with respect to $K$ examples \n",
        "\n",
        "6: $\\quad$ Compute adapted parameters with gradient descent: $\\theta_{i}^{\\prime}=\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}\\left(f_{\\theta}(\\mathcal{T}_{inner})\\right)$\n",
        "\n",
        "7: $\\quad$ Update $\\theta \\leftarrow \\theta-\\beta \\nabla_{\\theta_{i}^{\\prime}} \\mathcal{L}\\left(f_{\\theta_{i}^{\\prime}} \\left(\\mathcal{T}_{outer}\\right)\\right)$\n",
        "\n",
        "8: end while\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cayrPooZMLQJ"
      },
      "source": [
        "mtype='MAML'\n",
        "train_loader, val_loader, test_loader, c_dim, classes = get_data(dir=dir, mtype=mtype)\n",
        "\n",
        "make_example(train_loader=train_loader, classes=classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVVcU3nK69gl"
      },
      "source": [
        "if First_Train: model, MAML_loss_tracker, MAML_val_topks, MAML_val_accs, epoch_tracker = train(n_epochs=n_epochs, \n",
        "                                                                learning_rate=MAML_learning_rate1, \n",
        "                                                                learning_rate1=MAML_learning_rate2, \n",
        "                                                                train_loader=train_loader, \n",
        "                                                                val_loader=val_loader, \n",
        "                                                                c_dim=c_dim, \n",
        "                                                                mtype=mtype)\n",
        "else: \n",
        "    model = get_model(mtype)\n",
        "    MAML_loss_tracker=np.load(dir+mtype+'_loss_tracker.npy')\n",
        "    MAML_val_topks=np.load(dir+mtype+'_val_topks.npy')\n",
        "    MAML_val_accs=np.load(dir+mtype+'_val_accs.npy')\n",
        "    epoch_tracker=np.load(dir+'epoch_tracker.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWdiT2447L9q"
      },
      "source": [
        "make_test(model=model, testloader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQpW2D5LlyBf"
      },
      "source": [
        "# MAML n taskbatch and first order aproximation correction with inner L2\n",
        "\n",
        "Require: $p(\\mathcal{T}):$ distribution over tasks \n",
        "\n",
        "Require: $\\alpha, \\beta:$ step size hyperparameters\n",
        "\n",
        "1: randomly initialize $\\theta$\n",
        "\n",
        "2: while not done $\\mathbf{d o}$\n",
        "\n",
        "3: $\\quad$ Sample batch of tasks $\\mathcal{T}_{i} \\sim p(\\mathcal{T})$\n",
        "\n",
        "4: $\\quad$ for all $\\mathcal{T}_{i}$ do \n",
        "\n",
        "5: $\\quad$ $\\quad$ Evaluate $\\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta}\\right)$ with respect to $K$ examples \n",
        "\n",
        "6: $\\quad$ $\\quad$ Compute adapted parameters with gradient descent: $\\theta_{i}^{\\prime}=\\theta-\\alpha \\nabla_{\\theta} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta}\\right) + ||\\theta||_2$\n",
        "\n",
        "7: $\\quad$ end for\n",
        "\n",
        "8: $\\quad$Update $\\theta \\leftarrow \\theta-\\beta \\sum_{\\mathcal{T}_{i} \\sim p(\\mathcal{T})} \\nabla_{\\theta_{i}^{\\prime}} \\mathcal{L}_{\\mathcal{T}_{i}}\\left(f_{\\theta_{i}^{\\prime}})\\right)$\n",
        "\n",
        "9: end while\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiIa4IU5xto3"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKARo6YIl5yv"
      },
      "source": [
        "mtype='MAML'\n",
        "train_loader, val_loader, test_loader, c_dim, classes = get_data(dir=dir, mtype=mtype)\n",
        "\n",
        "make_example(train_loader=train_loader, classes=classes)\n",
        "mtype='MAML2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTbTmN0SmEHh"
      },
      "source": [
        "if First_Train: model, MAML_loss_tracker2, MAML_val_topks2, MAML_val_accs2, epoch_tracker, = train2(n_epochs=n_epochs, \n",
        "                                                                learning_rate=MAML2_learning_rate2, \n",
        "                                                                learning_rate1=MAML2_learning_rate2, \n",
        "                                                                train_loader=train_loader, \n",
        "                                                                val_loader=val_loader, \n",
        "                                                                c_dim=c_dim, \n",
        "                                                                mtype=mtype,)\n",
        "else: \n",
        "    model = get_model(mtype)\n",
        "    MAML_loss_tracker2=np.load(dir+mtype+'_loss_tracker.npy')\n",
        "    MAML_val_topks2=np.load(dir+mtype+'_val_topks.npy')\n",
        "    MAML_val_accs2=np.load(dir+mtype+'_val_accs.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POYQj09OvHbB"
      },
      "source": [
        "make_test(model=model, testloader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMcY3zQPdrJK"
      },
      "source": [
        "## RESISC CNN with Augs\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBrfnYVfdd5e"
      },
      "source": [
        "mtype='CNN_Aug'\n",
        "train_loader, val_loader, test_loader, c_dim, classes = get_data(dir=dir, mtype=mtype)\n",
        "\n",
        "if First_Train: model, CNN_Aug_loss_tracker, CNN_Aug_val_topks, CNN_Aug_val_accs, epoch_tracker = train(n_epochs=n_epochs, \n",
        "                                                                learning_rate=CNN_learning_rate, \n",
        "                                                                learning_rate1=CNN_learning_rate, \n",
        "                                                                train_loader=train_loader, \n",
        "                                                                val_loader=val_loader, \n",
        "                                                                c_dim=c_dim, \n",
        "                                                                mtype=mtype)\n",
        "else: \n",
        "    model = get_model(mtype)\n",
        "    CNN_Aug_loss_tracker=np.load(dir+mtype+'_loss_tracker.npy')\n",
        "    CNN_Aug_val_topks=np.load(dir+mtype+'_val_topks.npy')\n",
        "    CNN_Aug_val_accs=np.load(dir+mtype+'_val_accs.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5LMBVyhddUP"
      },
      "source": [
        "make_test(model=model, testloader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7LZFPmrFnXU"
      },
      "source": [
        "## RESISC CNN no Augs\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC3Gm029Fn5Z"
      },
      "source": [
        "mtype='CNN'\n",
        "train_loader, val_loader, test_loader, c_dim, classes = get_data(dir=dir, mtype=mtype)\n",
        "\n",
        "if First_Train: model, CNN_loss_tracker, CNN_val_topks, CNN_val_accs, epoch_tracker = train(n_epochs=n_epochs, \n",
        "                                                                learning_rate=CNN_learning_rate, \n",
        "                                                                learning_rate1=CNN_learning_rate, \n",
        "                                                                train_loader=train_loader, \n",
        "                                                                val_loader=val_loader, \n",
        "                                                                c_dim=c_dim, \n",
        "                                                                mtype=mtype,\n",
        "                                                                augs=False)\n",
        "else: \n",
        "    model = get_model(mtype)\n",
        "    CNN_loss_tracker=np.load(dir+mtype+'_loss_tracker.npy')\n",
        "    CNN_val_topks=np.load(dir+mtype+'_val_topks.npy')\n",
        "    CNN_val_accs=np.load(dir+mtype+'_val_accs.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtvRy9hYFofR"
      },
      "source": [
        "make_test(model=model, testloader=test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unkZk4GZuimF"
      },
      "source": [
        "## RESISC CHARTS\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IMop6hcddro"
      },
      "source": [
        "make_train_plot(MAML_loss_tracker, CNN_Aug_loss_tracker, CNN_loss_tracker, MAML_val_accs, CNN_Aug_val_accs, CNN_val_accs, MAML_val_topks, CNN_Aug_val_topks, CNN_val_topks, epoch_tracker,MAML_loss_tracker2,MAML_val_accs2,MAML_val_topks2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LZNyxMlddjY"
      },
      "source": [
        "make_two_plots(epoch_tracker, MAML_loss_tracker, CNN_Aug_loss_tracker, CNN_loss_tracker, MAML_val_accs, CNN_Aug_val_accs, CNN_val_accs,MAML_loss_tracker2,MAML_val_accs2,MAML_val_topks2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofi4ngKHLmtn"
      },
      "source": [
        "## UC MERCED\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYDwEsFrLtbJ"
      },
      "source": [
        "UC_MAML_learning_rate = 0.0006\n",
        "UC_CNN_learning_rate = 0.0006\n",
        "UC_epochs = 3\n",
        "UC_batch_size = 32\n",
        "\n",
        "UC_data = np.load(dir+'UCMerced_images96.npy')\n",
        "UC_labels = np.load(dir+'UCMerced_classes.npy')\n",
        "UC_classes = np.load(dir+'UCMerced_class_names.npy')\n",
        "\n",
        "UC_train_data, UC_test_data, UC_train_labels, UC_test_labels = train_test_split(UC_data, UC_labels, test_size = 0.8, stratify = UC_labels)\n",
        "\n",
        "UCtrainset = make_dataset(UC_train_data, UC_train_labels)\n",
        "UCtrain_loader = DataLoader(UCtrainset, batch_size=int(batch_size), shuffle=True)\n",
        "\n",
        "UCtestset = make_dataset(UC_test_data, UC_test_labels)\n",
        "UCtest_loader = torch.utils.data.DataLoader(UCtestset, batch_size, drop_last = True, shuffle=True)\n",
        "\n",
        "def UC_get_data():\n",
        "    UC_data = np.load(dir+'UCMerced_images96.npy')\n",
        "    UC_labels = np.load(dir+'UCMerced_classes.npy')\n",
        "    UC_classes = np.load(dir+'UCMerced_class_names.npy')\n",
        "\n",
        "    UC_train_data, UC_test_data, UC_train_labels, UC_test_labels = train_test_split(UC_data, UC_labels, test_size = 0.8, stratify = UC_labels)\n",
        "\n",
        "    UCtrainset = make_dataset(UC_train_data, UC_train_labels)\n",
        "    UCtrain_loader = DataLoader(UCtrainset, batch_size=int(batch_size), shuffle=True)\n",
        "\n",
        "    UCtestset = make_dataset(UC_test_data, UC_test_labels)\n",
        "    UCtest_loader = torch.utils.data.DataLoader(UCtestset, batch_size, drop_last = True, shuffle=True)\n",
        "    return UCtrain_loader, UCtest_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Du65kG5tVef"
      },
      "source": [
        "def UC_train(model, trainloader, epochs=3, lr=0.0006):\n",
        "    loss_tracker, epoch_tracker = [], []\n",
        "    #model.load('UC')\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "    for epoch in range(1, epochs+1):\n",
        "        losses = 0\n",
        "        for X, y in trainloader:\n",
        "            y = one_hot_embedding(y.to(device), len(UC_classes)).float()\n",
        "            X = X.float().to(device)\n",
        "            for param in model.parameters(): param.grad = None\n",
        "            yhat = model(X)\n",
        "            pred_loss = bce_loss(yhat, y)\n",
        "            pred_loss.backward()\n",
        "            optimizer.step()\n",
        "            losses += pred_loss.item()\n",
        "        epoch_tracker.append(epoch)\n",
        "        loss_tracker.append(losses)\n",
        "        print(losses)\n",
        "    return model, epoch_tracker, loss_tracker    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7zMruiyLtXs"
      },
      "source": [
        "mtype='MAML'\n",
        "model = get_model(mtype=mtype,load=True)\n",
        "\n",
        "if Freeze:\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fce = nn.Linear(1024, 21)\n",
        "model = model.to(device)\n",
        "#model.save('UC'+mtype)\n",
        "\n",
        "model, epoch_tracker, MAML_loss_tracker = UC_train(model=model, trainloader=UCtrain_loader, epochs=UC_epochs, lr=UC_MAML_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wepsXFE7LtRj"
      },
      "source": [
        "make_test(model=model, testloader=UCtest_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axX-Telin6aG"
      },
      "source": [
        "mtype='MAML2'\n",
        "model = get_model(mtype=mtype,load=True)\n",
        "\n",
        "if Freeze:\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fce = nn.Linear(1024, 21)\n",
        "model = model.to(device)\n",
        "#model.save('UC'+mtype)\n",
        "\n",
        "model, epoch_tracker, MAML_loss_tracker2 = UC_train(model=model, trainloader=UCtrain_loader, epochs=UC_epochs, lr=UC_MAML_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0XSuYyJn82R"
      },
      "source": [
        "make_test(model=model, testloader=UCtest_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11SceGJmLtO3"
      },
      "source": [
        "mtype='CNN'\n",
        "model = get_model(mtype=mtype,load=True)\n",
        "\n",
        "if Freeze:\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fce = nn.Linear(1024, 21)\n",
        "model = model.to(device)\n",
        "#model.save('UC'+mtype)\n",
        "\n",
        "model, epoch_tracker, CNN_loss_tracker = UC_train(model=model, trainloader=UCtrain_loader, epochs=UC_epochs, lr=UC_CNN_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tueMpx6iLs3O"
      },
      "source": [
        "make_test(model=model, testloader=UCtest_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDoGbeQ0KA4y"
      },
      "source": [
        "mtype='CNN_Aug'\n",
        "model = get_model(mtype=mtype,load=True)\n",
        "\n",
        "if Freeze:\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "model.fce = nn.Linear(1024, 21)\n",
        "model = model.to(device)\n",
        "#model.save('UC'+mtype)\n",
        "\n",
        "model, epoch_tracker, CNN_Aug_loss_tracker = UC_train(model=model, trainloader=UCtrain_loader, epochs=UC_epochs, lr=UC_CNN_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwNe13xWKAri"
      },
      "source": [
        "make_test(model=model, testloader=UCtest_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD5NUS3wLtKv"
      },
      "source": [
        "basic_train_plot(epoch_tracker, MAML_loss_tracker, CNN_Aug_loss_tracker, CNN_loss_tracker,MAML_loss_tracker2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoCj5Wd8XuK_"
      },
      "source": [
        "## thoughts on UC mercer\n",
        "\n",
        "To make our results stronger in the code below I trined a fresh CNN,(Should We also do this for other optimization schemes?) on the UC data from scratch with the same params. It is to note that each model gets to about 100 VAL very fast. When looking at the testing data it overfit. I belive this makes our results stronger because we can for sure say some information was passed from the pretraining on RESIC, where it would have otherwise overfit with the same amout of data. It may be worth Rethinking the ratios of testing and training split for UC mercer data as well..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvBt7aOwYkWs"
      },
      "source": [
        "def fresh_train_UC(mtype,load):\n",
        "    model = get_model(mtype=mtype,load=load)\n",
        "    UC_MAML_learning_rate = 0.0006\n",
        "    UC_CNN_learning_rate = 0.0006\n",
        "    UC_epochs = 5\n",
        "    UC_batch_size = 32\n",
        "    UCtrain_loader,UCtest_loader = UC_get_data()\n",
        "    model.fce = nn.Linear(1024, 21)\n",
        "    model = model.to(device)\n",
        "    #model.save('UC'+mtype)\n",
        "    if mtype == 'CNN':\n",
        "        print(mtype)\n",
        "        model, epoch_tracker, CNN_Aug_loss_tracker = UC_train(model=model, trainloader=UCtrain_loader, epochs=UC_epochs, lr=UC_CNN_learning_rate)\n",
        "        acc, topk = make_test(model=model, testloader=UCtest_loader)\n",
        "    elif mtype == \"MAML\":\n",
        "        print(mtype)\n",
        "        model, _, _, _, _=  train(UC_epochs, UC_CNN_learning_rate, UC_CNN_learning_rate, UCtrain_loader, UCtest_loader, c_dim, mtype, augs=True)\n",
        "        acc, topk = make_test(model=model, testloader=UCtest_loader)\n",
        "\n",
        "    elif mtype == 'MAML2':\n",
        "        print(mtype)\n",
        "        model, _, _, _, _ =  train2(UC_epochs, UC_CNN_learning_rate, UC_CNN_learning_rate, UCtrain_loader, UCtest_loader, c_dim, mtype)\n",
        "        acc, topk = make_test(model=model, testloader=UCtest_loader)\n",
        "    return acc,topk\n",
        "\n",
        "fresh_train_UC('CNN')\n",
        "fresh_train_UC('MAML')\n",
        "fresh_train_UC('MAML2')\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F5BOpX07VDA"
      },
      "source": [
        "## Scrap Work Area \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6y3pEe-0e2O"
      },
      "source": [
        "#GRAD DIAGNOSTICS\n",
        "'''\n",
        "print(\"PARAMS YOU ARE CHECKING\")\n",
        "for pA, pB in zip(CNN.parameters(), CNN1.parameters()):\n",
        "  print((pA == pB).all())\n",
        "print(\"GRAD YOU ARE CHECKING\")\n",
        "for pA, pB in zip(CNN.parameters(), CNN1.parameters()):\n",
        "  print((pA.grad == pB.grad))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSdAAONZe1Y_"
      },
      "source": [
        "def trainmodel(dir,mtype,n_epochs,learning_rate_1,learning_rate_2):\n",
        "    train_loader, val_loader, test_loader, c_dim, classes = get_data(dir=dir, mtype=mtype)\n",
        "    make_example(train_loader=train_loader, classes=classes)\n",
        "    if mtype=='CNN' or mtype == \"MAML\":\n",
        "        model, loss_tracker, val_topk_tracker, val_acc_tracker, epoch_tracker = train(n_epochs=n_epochs, \n",
        "                                                                    learning_rate=learning_rate_1, \n",
        "                                                                    learning_rate1=learning_rate_2, \n",
        "                                                                    train_loader=train_loader, \n",
        "                                                                    val_loader=val_loader, \n",
        "                                                                    c_dim=c_dim, \n",
        "                                                                    mtype=mtypeprint_stride1=1)\n",
        "    elif mtype == 'MAML2'\n",
        "         model, loss_tracker, val_topk_tracker, val_acc_tracker, epoch_tracker = train2(n_epochs=n_epochs, \n",
        "                                                                    learning_rate=learning_rate_1, \n",
        "                                                                    learning_rate1=learning_rate_2, \n",
        "                                                                    train_loader=train_loader, \n",
        "                                                                    val_loader=val_loader, \n",
        "                                                                    c_dim=c_dim, \n",
        "                                                                    mtype=mtype,print_stride1=1)\n",
        "    return model,test_loader,loss_tracker,val_topk_tracker,val_acc_tracker,epoch_tracker\n",
        "\n",
        "def AVG_n_model(dir,mtype,n_epochs,learning_rate_1,learning_rate_2,runs=5,UC_or_RES):\n",
        "    list_loss_tracker,list_val_topk_tracker,list_val_acc_tracker,testacc,testtopk = [],[],[],[],[]\n",
        "    for _ in range(runs):\n",
        "        if UC_or_RES=='RES'\n",
        "            model,test_loader,loss_tracker,val_topk_tracker,val_acc_tracker,epoch_tracker = trainmodel(dir,mtype,n_epochs,learning_rate_1,learning_rate_2)\n",
        "            list_loss_tracker.append(loss_tracker)\n",
        "            list_val_topk_tracker.append(val_topk_tracker)\n",
        "            list_val_acc_tracker.append(val_acc_tracker)\n",
        "\n",
        "            test_acc, test_topk  = make_test(model,test_loader)\n",
        "            testacc.append(test_acc)\n",
        "            testtopk.append(test_topk)\n",
        "        else:\n",
        "            testacc1 , testtopk1 = fresh_train_UC(mtype,load=True)\n",
        "            testacc.append(testacc1)\n",
        "            testtopk.append(testtopk1)\n",
        "\n",
        "\n",
        "    avg_loss = numpy.mean(numpy.array(list_loss_tracker), axis=0)\n",
        "    avg_valtopk = numpy.mean(numpy.array(list_val_topk_tracker), axis=0)\n",
        "    avg_valacc = numpy.mean(numpy.array(val_acc_tracker), axis=0)\n",
        "    \n",
        "    test__valacc = numpy.mean(numpy.array(testacc), axis=0)\n",
        "    test___valacc = numpy.mean(numpy.array(testtopk), axis=0)\n",
        "\n",
        "    return avg_loss,avg_valtopk,avg_valacc,test__valacc,test___valacc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}